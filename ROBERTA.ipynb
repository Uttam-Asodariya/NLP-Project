{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' -Use dynamic mask, instead of static mask\\n     -Remove the NSP task and train using the MLM task\\n     -Train with large batch size\\n     -use byte-level BPE(BBPE) as a tokenizer\\n     '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RoBERTa is BERT variation but following changes\n",
    "''' -Use dynamic mask, instead of static mask\n",
    "     -Remove the NSP task and train using the MLM task\n",
    "     -Train with large batch size\n",
    "     -use byte-level BPE(BBPE) as a tokenizer\n",
    "     '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOP basically a classification task, classify pair of words belongs to positive or negative class(Order of word not swapped)\n",
    "#or negative class(order of word swapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel, RobertaModel, RobertaTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.25.1\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta=RobertaModel.from_pretrained('roberta-base',output_hidden_states=True)\n",
    "roberta.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"C:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\NLP-Dataset.csv\"\n",
    "df=pd.read_csv(path,sep=None,engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    My name is uttam asodariya\n",
       "1        I am ambitious student\n",
       "2             I like travelling\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=df['sentence'].loc[0:2]\n",
    "test_data=df['sentence'].loc[3:]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.00380495,  0.0066697 , -0.0023425 , ..., -0.00875238,\n",
       "         -0.00319631, -0.00455428],\n",
       "        [-0.00872644,  0.00553709,  0.00703505, ..., -0.00305844,\n",
       "         -0.00150916, -0.01476572],\n",
       "        [-0.00223564,  0.006395  ,  0.00709376, ..., -0.02263761,\n",
       "         -0.001949  , -0.01004509],\n",
       "        ...,\n",
       "        [-0.00790431,  0.00265047,  0.01174918, ..., -0.03812897,\n",
       "         -0.01115266, -0.0071437 ],\n",
       "        [ 0.00882291,  0.00030284,  0.00789095, ..., -0.02823584,\n",
       "         -0.0073701 , -0.01307125],\n",
       "        [-0.00331656,  0.00618941, -0.00465911, ..., -0.01265754,\n",
       "         -0.00314743, -0.00760065]], dtype=float32),\n",
       " array([[-0.00288601,  0.00594991, -0.00174751, ..., -0.00520786,\n",
       "         -0.00245082, -0.00411506],\n",
       "        [-0.00365515,  0.00737569, -0.0091724 , ...,  0.02620407,\n",
       "          0.00355022, -0.01840989],\n",
       "        [ 0.00868383,  0.01075837,  0.01084779, ...,  0.00615936,\n",
       "          0.00550099, -0.01763579],\n",
       "        [ 0.00626601, -0.01360092, -0.00530282, ...,  0.00873719,\n",
       "          0.00667668,  0.01426462],\n",
       "        [ 0.00721669, -0.00813592, -0.00638489, ...,  0.00175802,\n",
       "          0.00706382,  0.00287186],\n",
       "        [-0.00198791,  0.00503294, -0.00434578, ..., -0.00898868,\n",
       "         -0.00148906, -0.00730091]], dtype=float32),\n",
       " array([[-0.00483858,  0.00837755, -0.00146502, ..., -0.00531675,\n",
       "         -0.0038789 , -0.00340519],\n",
       "        [-0.00461756,  0.00813541, -0.01121358, ...,  0.03478093,\n",
       "          0.00469687, -0.02276029],\n",
       "        [-0.00136334,  0.01849529,  0.01031234, ...,  0.03935553,\n",
       "          0.01467698, -0.01262126],\n",
       "        [ 0.01207952,  0.00069084, -0.00229256, ...,  0.0197608 ,\n",
       "          0.00989127, -0.02420804],\n",
       "        [-0.00431864,  0.00874991, -0.00392794, ..., -0.00937332,\n",
       "         -0.00359053, -0.00747509]], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Word_embedding(dataframe):\n",
    "    encoded_tensor=[]\n",
    "    for i in dataframe:\n",
    "        input = torch.tensor(tokenizer.encode(i, add_special_tokens=True)).unsqueeze(0)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = roberta(input)\n",
    "            last_hidden_states=output[0]\n",
    "         \n",
    "        last_hidden_states=np.array(last_hidden_states)\n",
    "        sum_of_encoders=np.sum(last_hidden_states,axis=0)\n",
    "        tensor=sum_of_encoders/12 \n",
    "# we have 12 encoder output(length of 768) per token. Therefor average it\n",
    "        encoded_tensor.append(tensor)\n",
    "    return encoded_tensor\n",
    "\n",
    "         \n",
    "encoded_tensor=Word_embedding(train_data)      \n",
    "encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(encoded_tensor): \n",
    "    encoded_tensor1=encoded_tensor[0]\n",
    "    encoded_tensor2=encoded_tensor[1]\n",
    "    \n",
    "    print('cosine similarity between sentence 1&2', cosine_similarity(encoded_tensor1,\n",
    "                                                                        encoded_tensor2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc9c18c49ecff0086ebd087ab3cf18fc94c56d493e506b9bf33d86b94d3b3ea4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
