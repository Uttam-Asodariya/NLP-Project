{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' -Use dynamic mask, instead of static mask\\n     -Remove the NSP task and train using the MLM task\\n     -Train with latge batch size\\n     -use byte-level BPE(BBPE) as a tokenizer\\n     '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RoBERTa is BERT variation but following changes\n",
    "''' -Use dynamic mask, instead of static mask\n",
    "     -Remove the NSP task and train using the MLM task\n",
    "     -Train with latge batch size\n",
    "     -use byte-level BPE(BBPE) as a tokenizer\n",
    "     '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOP basically a classification task, classify pair of words belongs to positive or negative class(Order of word not swapped)\n",
    " #or negative class(order of word swapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel, RobertaModel, RobertaTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.25.1\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta=RobertaModel.from_pretrained('roberta-base',output_hidden_states=True)\n",
    "roberta.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"C:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\NLP-Dataset.csv\"\n",
    "df=pd.read_csv(path,sep=None,engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    My name is uttam asodariya\n",
       "1        I am ambitious student\n",
       "2             I like travelling\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=df['sentence'].loc[0:2]\n",
    "test_data=df['sentence'].loc[3:]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e09a6515af6472bb5e07eb3f8576be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/445 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9898d23948ff4e32b835aeb434b22be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7717e63f2eb34be9b15ae571779dfa3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b355b29160d34184bc2e4fdd8543602e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/603k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba9fea7e5ef43578f5648fccdc55191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62c64aebbdd4e2da3b6668f27394a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298b35da41174d2dad9e541273ea6c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a9473dbc884e159fce00ff1b0da7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d84ef6ea5542b78dba974142001556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\uttam/.cache\\torch\\sentence_transformers\\roberta-base. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\uttam/.cache\\torch\\sentence_transformers\\roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9542, 0.9307],\n",
       "        [0.9542, 1.0000, 0.9757],\n",
       "        [0.9307, 0.9757, 1.0000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cosine similarity\n",
    "model = SentenceTransformer('roberta-base')\n",
    "embedding=model.encode(train_data)\n",
    "embedding.shape\n",
    "cosine_scores = util.cos_sim(embedding, embedding)\n",
    "cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00380495,  0.0066697 , -0.0023425 , ..., -0.00875238,\n",
       "        -0.00319631, -0.00455428],\n",
       "       [-0.00872644,  0.00553709,  0.00703505, ..., -0.00305844,\n",
       "        -0.00150916, -0.01476572],\n",
       "       [-0.00223564,  0.006395  ,  0.00709376, ..., -0.02263761,\n",
       "        -0.001949  , -0.01004509],\n",
       "       ...,\n",
       "       [-0.00790431,  0.00265047,  0.01174918, ..., -0.03812897,\n",
       "        -0.01115266, -0.0071437 ],\n",
       "       [ 0.00882291,  0.00030284,  0.00789095, ..., -0.02823584,\n",
       "        -0.0073701 , -0.01307125],\n",
       "       [-0.00331656,  0.00618941, -0.00465911, ..., -0.01265754,\n",
       "        -0.00314743, -0.00760065]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Word_embedding(dataframe):\n",
    "    encoded_tensor=[]\n",
    "    for i in dataframe:\n",
    "        input = torch.tensor(tokenizer.encode(i, add_special_tokens=True)).unsqueeze(0)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = roberta(input)\n",
    "            last_hidden_states=output[0]\n",
    "         \n",
    "        last_hidden_states=np.array(last_hidden_states)\n",
    "        sum_of_encoders=np.sum(last_hidden_states,axis=0)\n",
    "        tensor=sum_of_encoders/12 \n",
    "# we have 12 encoder output(length of 768) per token. Therefor average it\n",
    "        encoded_tensor.append(tensor)\n",
    "    return encoded_tensor\n",
    "\n",
    "         \n",
    "encoded_tensor=Word_embedding(train_data)      \n",
    "encoded_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(encoded_tensor): \n",
    "    encoded_tensor1=encoded_tensor[0]\n",
    "    encoded_tensor2=encoded_tensor[1]\n",
    "    \n",
    "    print('cosine similarity between sentence 1&2', cosine_similarity(encoded_tensor1,\n",
    "                                                                        encoded_tensor2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between sentence 1&2 [[0.9987668  0.8235894  0.80704814 0.7604036  0.80606145 0.99452466]\n",
      " [0.82093227 0.93390083 0.8927067  0.8290058  0.8638848  0.82419807]\n",
      " [0.6305332  0.80441976 0.84412813 0.7978529  0.8049452  0.6363224 ]\n",
      " [0.68474334 0.8603505  0.9089815  0.8269222  0.83296317 0.690841  ]\n",
      " [0.72782403 0.77462906 0.8077069  0.7993326  0.8073348  0.73623234]\n",
      " [0.71977025 0.7814284  0.8028483  0.7847976  0.7868681  0.73200077]\n",
      " [0.75327784 0.84998053 0.87524927 0.8394208  0.8666037  0.764046  ]\n",
      " [0.6799656  0.750145   0.8050009  0.77797407 0.7694006  0.6898856 ]\n",
      " [0.6359     0.7449647  0.7940486  0.7899357  0.7950175  0.64920956]\n",
      " [0.7375224  0.82850647 0.8224846  0.81816655 0.84501475 0.7474714 ]\n",
      " [0.7651556  0.86588365 0.8757618  0.8586716  0.895524   0.7756463 ]\n",
      " [0.9959017  0.82889813 0.8149374  0.767716   0.8180362  0.99801546]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc9c18c49ecff0086ebd087ab3cf18fc94c56d493e506b9bf33d86b94d3b3ea4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
