{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstratct\n",
    "\n",
    "- There are numerous illnesses connected to memory loss in medicine. It includes Alzheimer's disease. To effectively treat this type of sickness, early detection is crucial. The domains of deep learning (DL) and natural language processing (NLP) can point us in the right path for a solution. Word embedding, a process that turns words into vectors, is made possible by NLP. These word embeddings are accurately classified based on the following classes: Alzeimer disease (AD) and healthy control. (HC). Word embedding takes a lot of effort and is computationally expensive. To evaluate the effectiveness of classification models, it is also crucial to concentrate on precision and recall value. The accuracy of the model can yet be improved through the application of new approaches. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- Good mental health is necessary for the development of a better world. Although medical research has advanced significantly, there are still some diseases that cannot be detected early enough to be treated. The classification method can identify such disorders using a combination of **natural language processing** (NLP) and **deep learning** (DL). Because of their busy schedules, people occasionally neglect to sleep, which causes them to develop Alzeimer disease as they get older.\n",
    "\n",
    "- Alzheimer's is one of the diseases that is hard to detect in its earlier phases or even in the present moment. Fortunately, data science has the solution. A combination of natural language processing and deep learning can be used to detect the disease. The Boston cookie theft picture was shown to people from both categories, and based on the description of the picture, Alzeihmer can be classified. Sentences can be transformed into word embedding by NLP, where the representation of words is converted into vectors of real numbers. This can be done by BERT (bidirectional encoder representations from transformers). Bert has different variations, such as Roberta and Electra.\n",
    "\n",
    "- ROBERTA (Robustly Optimized BERT-Pretraining Approach) is a powerful model for word embedding. It transforms each sentence into a token, and these tokens are used by the model for word embedding in the encoder phase.\n",
    "\n",
    "- Support vwctor classification (SVC) model is useful for this high-dimensional word embedding, as it has a huge amount of uncertainty in the data. Accuracy is a way more crucial in the medical field. There are some limitations which cannot be overlooked. The right way of word embedding and feature engineering can help to improve classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment for this notbook\n",
    "import torch\n",
    "\n",
    "from transformers import RobertaModel, RobertaModel, RobertaTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from scipy.stats import entropy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types\n",
    "\n",
    "- There are two types of datasets. One with sentences collected from AD and HC patients, and the second data type contains the health description of the same patients with their ID. The first data type is more important to classify Alzeimer, which was first used for the word embedding and later for classification. These data were collected based on image descriptions of patients affected by Alzeihmer and those not affected by Alzeihmer.\n",
    "\n",
    "- The second data type has a couple of attributes such as ID_Pitt, Gender, Age, Scholarity, MMSE_Pitt, and Session. The goal of this dataset is to analyze which parameter is mostly connected with Alzeimer's disease. In addition, it has the same two classes, AD and HC.\n",
    "\n",
    "\n",
    "### Pre-processing\n",
    "\n",
    "- There are many csv files according to the patients descriptions, and therefore it is important to convert them into an appropriate format for word embedding. Through the combination of all csv files, make it one and add a label to identify which is Alzeimer disease (AD) and healthy control (HC). However, it is also important to separate 30% of the files at the beginning for the test set. This helps to stop mixture of senteces from train set and test set. Because it is essential to maintaining the uniqueness of patients' scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all csv files of AD_train and add label column\n",
    "AD_train=[]\n",
    "path=r\"C:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\Data\\AD_train\"\n",
    "csv_files=glob.glob(path+'/*.csv')\n",
    "for f in csv_files:\n",
    "    df1 = pd.read_csv(f)\n",
    "    df1 = df1.loc[:, ~df1.columns.str.contains('^Unnamed')]\n",
    "    AD_train.append(df1)\n",
    "\n",
    "\n",
    "df_AD_train=pd.concat(AD_train,axis=0,ignore_index=True)\n",
    "label_AD=0 #AD : 0 label\n",
    "df_AD_train['label']=label_AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all csv files of AD_test and add label column\n",
    "AD_test=[]\n",
    "path=r\"C:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\Data\\AD_train\\AD_Test\"\n",
    "csv_files=glob.glob(path+'/*.csv')\n",
    "for f in csv_files:\n",
    "    df1 = pd.read_csv(f)\n",
    "    df1 = df1.loc[:, ~df1.columns.str.contains('^Unnamed')]\n",
    "    AD_test.append(df1)\n",
    "\n",
    "\n",
    "df_AD_test=pd.concat(AD_test,axis=0,ignore_index=True)\n",
    "label_AD=0 #AD : 0 label\n",
    "df_AD_test['label']=label_AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all csv files of HC_train and add label column\n",
    "HC_train=[]\n",
    "path=r\"C:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\Data\\HC_train\"\n",
    "csv_files=glob.glob(path+'/*.csv')\n",
    "for f in csv_files:\n",
    "    df2=pd.read_csv(f)\n",
    "    df2 = df2.loc[:, ~df2.columns.str.contains('^Unnamed')]\n",
    "    HC_train.append(df2)\n",
    "\n",
    "\n",
    "df_HC_train=pd.concat(HC_train,axis=0,ignore_index=True)\n",
    "label_HC=1 #HC : 1 label\n",
    "df_HC_train['label']=label_HC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all csv files of HC_test and add label column\n",
    "HC_test=[]\n",
    "path=r\"C:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\Data\\HC_train\\HC_Test\"\n",
    "csv_files=glob.glob(path+'/*.csv')\n",
    "for f in csv_files:\n",
    "    df2=pd.read_csv(f)\n",
    "    df2 = df2.loc[:, ~df2.columns.str.contains('^Unnamed')]\n",
    "    HC_test.append(df2)\n",
    "\n",
    "\n",
    "df_HC_test=pd.concat(HC_test,axis=0,ignore_index=True)\n",
    "label_HC=1 #HC : 1 label\n",
    "df_HC_test['label']=label_HC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatination of AD & HC train and similarly for test set\n",
    "df_train=pd.concat([df_AD_train,df_HC_train], ignore_index=True)\n",
    "df_test=pd.concat([df_AD_test,df_HC_test], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Furthermore, shuffled data samples are important to avoid overfitting and help create a robust classification model. After the pre-processing step, there are a total of 1968 samples in the train set and 649 samples in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train 1968 . length of test 649\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the mama's washin(g) dishes and the sink's run...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>that little girl .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and over in the window you see through the kit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh this is gonna be like looking at those arti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you want something else ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>I guess you take a big sledge hammer to that .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>and two children .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>the little girl is holding out her hand for a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>and then back there's a yard .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>three pieces of to eat on .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1968 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label\n",
       "0     the mama's washin(g) dishes and the sink's run...      1\n",
       "1                                    that little girl .      1\n",
       "2     and over in the window you see through the kit...      0\n",
       "3     oh this is gonna be like looking at those arti...      1\n",
       "4                             you want something else ?      1\n",
       "...                                                 ...    ...\n",
       "1963     I guess you take a big sledge hammer to that .      0\n",
       "1964                                 and two children .      0\n",
       "1965  the little girl is holding out her hand for a ...      1\n",
       "1966                     and then back there's a yard .      1\n",
       "1967                        three pieces of to eat on .      0\n",
       "\n",
       "[1968 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffling\n",
    "df_train = df_train.sample(frac=1,random_state=42,replace=True).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1,random_state=42,replace=True).reset_index(drop=True)\n",
    "print('length of train', len(df_train),'.','length of test', len(df_test)) # output of train and test length\n",
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "- - Sentence word embedding is the following stage. Tokenizers and the Roberta model are needed for word embedding. Sentences can be tokenized with the aid of a tokenizer. The 'G' character is used by Roberta's model to represent space as shown in the example below.\n",
    "\n",
    "\n",
    "- Example: 'My name is Khodiyar.\n",
    "           ['My','Gname', 'Gis', 'GKhodiyar'] =>  Tokenized sentence\n",
    "\n",
    "\n",
    "- Each tokenized phrase is transformed by the transformer into a unique id that the Roberta model can use for further processing. The tokenizer in this case inserts two extra tokens at the start and end of each sentence. To make each phrase distinct from the others.\n",
    "\n",
    "- To make this job more convenient, we set the length of the longest sentence—47 in our case—as the standard for all other sentences. Tokens that were not originally added are zero, so the model is unaffected. **model.config**  can be used to verify Roberta model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# loading of tokenizer and roberta model\n",
    "tokenizer=RobertaTokenizer.from_pretrained('roberta-base')\n",
    "Roberta = RobertaModel.from_pretrained('roberta-base',output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train=df_train['sentence'] # train\n",
    "sentences_test=df_test['sentence']  # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_pool={'sentences_train':sentences_train,'sentences_test':sentences_test} # train and test pool\n",
    "input_id_train=[] # list\n",
    "input_id_test=[]  # list\n",
    "max_len = 47\n",
    "\n",
    "# convert sentence into token\n",
    "for key in sentences_pool.keys():\n",
    "    input_ids = [torch.tensor(tokenizer.encode(sentence, add_special_tokens=True)) for sentence in sentences_pool[key]]\n",
    "\n",
    "    # Pad the sequences to have the same length\n",
    "    \n",
    "    padded_input = [torch.cat((input_id, torch.zeros((max_len - len(input_id)), dtype=torch.long))) for input_id in input_ids]\n",
    "    padded_input = torch.stack(padded_input).to(torch.int64) # tensor\n",
    "\n",
    "    if key=='sentences_train':\n",
    "        input_id_train.append(padded_input)\n",
    "    else :\n",
    "        input_id_test.append(padded_input)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Roberta's model is used following the conversion of words into the id. Keep the gradient off to accomplish word embedding. As a result, PyTorch won't record the processes used to calculate the word embeddings and won't employ them to calculate the gradients during backpropagation. We can decrease the memory usage and computation time needed during forward propagation by using **torch.no_grad()** when computing the word embeddings. We can also prevent any potential errors that may occur if we unintentionally attempt to compute gradients for the word embeddings. Additionally, turning off gradient tracking can help to increase the general training process' speed and effectiveness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_train=input_id_train[0] # get tensor from list\n",
    "\n",
    "#  word embedding of train\n",
    "with torch.no_grad():\n",
    "    output = Roberta(input_id_train)\n",
    "    last_hidden_state_train = output[2] # Word embedding can get by input for next part like forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_test=input_id_test[0] # get tensor from list\n",
    "\n",
    "# word embedding of test\n",
    "with torch.no_grad():\n",
    "    output = Roberta(input_id_test)\n",
    "    last_hidden_state_test = output[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A total of twelve embeddings are produced by the 12 encoders that make up the Roberta model. Every token is represented by a 768-valued vector. Combining encoders can result in the best encoding; in this instance, the classification objective was better served by the embedding of the final seven encoders. The dimension of each sentence is now [1,47,768]. Dimension reduction can be aided by the Entropy/Mean function. The tensor is transformed into a single array with dimension [1,47] by Entropy/Mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dimension of x_train is (without entorpy): torch.Size([1968, 47, 768])\n",
      "tensor(0.1589) tensor(0.2211)\n",
      "Final embedding shape x_train is (after entropy): torch.Size([1968, 47])\n",
      "----------\n",
      "Total dimension of x_test is (without entorpy): torch.Size([649, 47, 768])\n",
      "tensor(0.1589) tensor(0.2220)\n",
      "Final embedding shape x_test is (after entropy): torch.Size([649, 47])\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# find best embedding by combination of encoders and use entropy to convert sentence dimension from [1,47,768] to [1,768]\n",
    "\n",
    "'''\n",
    "Dimension: shape of whole dataset after embedding: [2617,47,768],\n",
    "           each sentence dimension: [1,47,768] shape \n",
    "           each word dimension: [1,768]\n",
    "'''\n",
    "x_train=[]\n",
    "x_test=[]\n",
    "embedding_pool={'x_train':last_hidden_state_train, 'x_test':last_hidden_state_test}\n",
    "\n",
    "for key in embedding_pool.keys():\n",
    "    final_embedding_sum=torch.stack(embedding_pool[key][-6:]).sum(0) # combine output of last six encoder\n",
    "    #--------------------------------------------\n",
    "#     n_samples = final_embedding_sum.shape[0]\n",
    "#     n_features = np.prod(final_embedding_sum.shape[1:])\n",
    "\n",
    "#     X = final_embedding_sum.reshape(n_samples, n_features)\n",
    "\n",
    "#     scaler = MinMaxScaler()\n",
    "#     scaler.fit(X)\n",
    "\n",
    "#    # Transform data using StandardScaler\n",
    "#     X_scaled = scaler.transform(X)\n",
    "\n",
    "#    # Reshape scaled data back to original tensor shape\n",
    "#     tensor_scaled = torch.tensor(X_scaled.reshape(n_samples, *final_embedding_sum.shape[1:]))\n",
    "\n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    print('Total dimension of', key, 'is (without entorpy):' ,final_embedding_sum.shape)\n",
    "\n",
    "    # mean\n",
    "    final_embedding = torch.mean(final_embedding_sum, dim=-1) # mean of last dimension\n",
    "\n",
    "    # entropy\n",
    "    # tensor_scaled=final_embedding_sum-(torch.min(final_embedding_sum))\n",
    "    # final_embedding = entropy(tensor_scaled, axis=-1) \n",
    "\n",
    "\n",
    "    print(torch.min(final_embedding),torch.max(final_embedding))\n",
    "    print('Final embedding shape', key, 'is (after entropy):',final_embedding.shape) # one dimensional single sentence of length 768\n",
    "    print('----------')\n",
    "    if key=='x_train':\n",
    "        x_train.append(final_embedding)\n",
    "    else:\n",
    "        x_test.append(final_embedding)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first part has been completed successfully. Now onwards, the classification  take into place. Achieved embedding used for the training along with its label.\n",
    "\n",
    "- A support vector classification (SVC) model works better compared to other model as it is a bit high-dimensional data. Appatently data is unlikely to normal distribution. Normalization techninuque is also used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification task\n",
    "x_train, x_test, y_train, y_test= x_train[0], x_test[0], df_train['label'], df_test['label']\n",
    " \n",
    "# Classification model: GaussianProcessClassifier \n",
    "SVC_model = SVC(gamma=2, C=1)\n",
    "model = make_pipeline(MinMaxScaler(), SVC_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train) # fit to the model\n",
    "y_pred=model.predict(x_test) # evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis\n",
    "\n",
    "-  Result accuracy generally lied between 56% and 70% with the mean/entropy technique. Recall (TP/TP+FN) percentage suggests how many items were correctly classified by our algorithm. Precesion (TP/TP+FP) describes how accurate a model is at identifying relevant items. Precision for class 0 (AD) is 63% and class 1 (HC) is 43%. Which means SVC is more reliable for classifying AD class rather than HC. Similarly for the precision model is more accurate towards AD than HC class. From the observation it can be said that classification model is able to make difference between  transcripts of Alzeihmer patient with normal person.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score :0.559322033898305\n",
      "-----classification report-------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.63      0.61       353\n",
      "           1       0.52      0.48      0.50       296\n",
      "\n",
      "    accuracy                           0.56       649\n",
      "   macro avg       0.55      0.55      0.55       649\n",
      "weighted avg       0.56      0.56      0.56       649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy score (result vary as dataset shuffle each time:54%-68%)\n",
    "print(f\"Accuracy score :{metrics.accuracy_score(y_test,y_pred)}\")\n",
    "\n",
    "# classification report\n",
    "print('-----classification report-------')\n",
    "class_labels=  [0,1] # AD : 0, HC : 1\n",
    "print(classification_report(y_test, y_pred, labels=class_labels)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- In conclusion, due of a busy schedule, people are sleeping less, which can occasionally lead to Alzheimer's. Memory loss may be lessened if Alzeihmer disease is identified earlier. The BERT variation model for word embedding named Roberta is the most potent. In order to obtain the greatest word embedding, it is crucial to choose a decent combination of encoders. This is the key element in accurate classification. Word embedding has a high dimensionality, and the mean/entropy function can be used to minimize dimensionality. The SVC model may be more suitable than others due to the high dimensionality of word embedding. Class 0 (AD) and class 1 (HC) achieved recall results of 63% and 48%, respectively.\n",
    "However, there are certain restrictions, such as the possibility that the combination of chosen encoders may not be an acceptable option for every word, which can prevent the model from becoming more universal. However, by altering the process used to create word embedding and locating a more precise categorization model, the accuracy can be increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc9c18c49ecff0086ebd087ab3cf18fc94c56d493e506b9bf33d86b94d3b3ea4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
