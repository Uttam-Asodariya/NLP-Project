{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstratct\n",
    "\n",
    "- In medical science, there are many diseases related to memory loss. Alzheimer's is one of them. It is important to detect this kind of disease at an early stage to cure it. The combination of the natural language processing (NLP) and deep learning (DL) fields can lead us in the direction of a solution. NLP can help transform words into vectors known as word embedding. The classification performed on these word embeddings to correctly classify them was based on Alzeimer disease (AD) and healthy control (HC). Word embedding is computationally expensive and consumes much time. It is also important to focus on precesion and recall value to check classification model intergrity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- For the creation of a better world, good mental health is required. Medical science has progressed outstandingly, but still, some diseases exist that cannot be identified earlier to be cured. Natural language processing (NLP) and deep learning (DL) combination can be used to identify such diseases by the classification algorithm.\n",
    "\n",
    "\n",
    "- Alzheimer's is one of the diseases that is hard to detect in its earlier phases or even in the present moment. Fortunately, data science has the solution. A combination of natural language processing and deep learning can be used to detect the disease. Sentences can be transformed into word embedding by NLP, where the representation of words is converted into vectors of real numbers. This can be done by BERT (bidirectional encoder representations from transformers). Berta has different variations, such as Roberta and Electra.\n",
    "\n",
    "\n",
    "- ROBERTA (Robustly Optimized BERT-Pretraining Approach) is a powerful model for word embedding. It transforms each sentence into a token, and these tokens are used by the model for word embedding in the encoder phase.\n",
    "\n",
    "- Gaussian process classification is useful for this high-dimensional word embedding, as it has a huge amount of uncertainty in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment for this notbook\n",
    "import torch\n",
    "\n",
    "from transformers import RobertaModel, RobertaModel, RobertaTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "from scipy.stats import entropy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "- There is two type of datasets. One with sentences and second with the patient health discription. First  type of data is used for this word embedding task. Data collected by image desciption of patients affected by Alzeihmer and helathy person.\n",
    "\n",
    "### Pre-processing\n",
    "\n",
    "- There are a couple of csv files based on the patients descriptions, and therefore it is important to convert them into an appropriate format for word embedding. Through the combination of all csv files, make it one and add a label to identify which is Alzeimer disease (AD) and healthy control (HC). Similarly, the same process was performed for the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all csv files of AD_train and add label column\n",
    "AD_train=[]\n",
    "path=r\"C:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\Data\\AD_train\"\n",
    "csv_files=glob.glob(path+'/*.csv')\n",
    "for f in csv_files:\n",
    "    df1 = pd.read_csv(f)\n",
    "    df1 = df1.loc[:, ~df1.columns.str.contains('^Unnamed')]\n",
    "    AD_train.append(df1)\n",
    "\n",
    "\n",
    "df_AD_train=pd.concat(AD_train,axis=0,ignore_index=True)\n",
    "label_AD=0 #AD : 0 label\n",
    "df_AD_train['label']=label_AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all csv files of AD_test and add label column\n",
    "AD_test=[]\n",
    "path=r\"C:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\Data\\AD_train\\AD_Test\"\n",
    "csv_files=glob.glob(path+'/*.csv')\n",
    "for f in csv_files:\n",
    "    df1 = pd.read_csv(f)\n",
    "    df1 = df1.loc[:, ~df1.columns.str.contains('^Unnamed')]\n",
    "    AD_test.append(df1)\n",
    "\n",
    "\n",
    "df_AD_test=pd.concat(AD_test,axis=0,ignore_index=True)\n",
    "label_AD=0 #AD : 0 label\n",
    "df_AD_test['label']=label_AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all csv files of HC_train and add label column\n",
    "HC_train=[]\n",
    "path=r\"C:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\Data\\HC_train\"\n",
    "csv_files=glob.glob(path+'/*.csv')\n",
    "for f in csv_files:\n",
    "    df2=pd.read_csv(f)\n",
    "    df2 = df2.loc[:, ~df2.columns.str.contains('^Unnamed')]\n",
    "    HC_train.append(df2)\n",
    "\n",
    "\n",
    "df_HC_train=pd.concat(HC_train,axis=0,ignore_index=True)\n",
    "label_HC=1 #HC : 1 label\n",
    "df_HC_train['label']=label_HC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all csv files of HC_test and add label column\n",
    "HC_test=[]\n",
    "path=r\"C:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\Data\\HC_train\\HC_Test\"\n",
    "csv_files=glob.glob(path+'/*.csv')\n",
    "for f in csv_files:\n",
    "    df2=pd.read_csv(f)\n",
    "    df2 = df2.loc[:, ~df2.columns.str.contains('^Unnamed')]\n",
    "    HC_test.append(df2)\n",
    "\n",
    "\n",
    "df_HC_test=pd.concat(HC_test,axis=0,ignore_index=True)\n",
    "label_HC=1 #HC : 1 label\n",
    "df_HC_test['label']=label_HC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatination of AD & HC train and similarly for test set\n",
    "df_train=pd.concat([df_AD_train,df_HC_train], ignore_index=True)\n",
    "df_test=pd.concat([df_AD_test,df_HC_test], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Furthermore, it is essential to shuffle data samples to avoid overfitting and also help create a robust classification model. After pre-processing step, There is total 1968 samples in train set and 649 samples in test set. It is important to separate the training and test sets in the beginning to avoid a mixture of sentences from one script to another, because it is essential to maintaining the uniqueness of patients' scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train 1968 . length of test 649\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I heard it might but I'm sayin(g) let's tell h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and the woman is standing in water .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the little girl's got her hand up for one .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wonder if he got a cookie .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>fell down .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>uhhuh .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>that's a mess .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>touching lip .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>doin(g) .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1968 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label\n",
       "0     I heard it might but I'm sayin(g) let's tell h...      0\n",
       "1                                                 yes .      0\n",
       "2                  and the woman is standing in water .      1\n",
       "3           the little girl's got her hand up for one .      1\n",
       "4                           wonder if he got a cookie .      0\n",
       "...                                                 ...    ...\n",
       "1963                                        fell down .      0\n",
       "1964                                            uhhuh .      1\n",
       "1965                                    that's a mess .      0\n",
       "1966                                     touching lip .      1\n",
       "1967                                          doin(g) .      0\n",
       "\n",
       "[1968 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffling\n",
    "df_train = df_train.sample(frac=1,random_state=0,replace=True).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1,random_state=0,replace=True).reset_index(drop=True)\n",
    "print('length of train', len(df_train),'.','length of test', len(df_test)) # output of train and test length\n",
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "- The next step is word embedding in sentences. For the word embedding, tokenizers and the Roberta model are required. Tokenizers are helpful for tokenizing sentences. Roberta model use 'G' character to represt space as described in given below example.\n",
    "\n",
    "\n",
    "Example: 'My name is Khodiyar.\n",
    "           ['My','Gname', 'Gis', 'GKhodiyar'] =>  Tokenized sentence\n",
    "\n",
    "\n",
    "- The transformer converts each tokenized word into a unique ID, which can be taken by the Roberta model for further processing. Here, the tokenizer adds two additional tokens at the beginning and end of each sentence. For the distinguish each sentence to each other.\n",
    "\n",
    "\n",
    "- For more convenience in this task we make same length for all sentences by taking into account length of longest sentence wich is 47 in our case. Extra-added tokens are zero, which will not affect the model. Roberta model configuration can be checked by **model.config**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# loading of tokenizer and roberta model\n",
    "tokenizer=RobertaTokenizer.from_pretrained('roberta-base')\n",
    "Roberta = RobertaModel.from_pretrained('roberta-base',output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train=df_train['sentence'] # train\n",
    "sentences_test=df_test['sentence']  # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_pool={'sentences_train':sentences_train,'sentences_test':sentences_test} # train and test pool\n",
    "input_id_train=[] # list\n",
    "input_id_test=[]  # list\n",
    "max_len = 47\n",
    "\n",
    "# convert sentence into token\n",
    "for key in sentences_pool.keys():\n",
    "    input_ids = [torch.tensor(tokenizer.encode(sentence, add_special_tokens=True)) for sentence in sentences_pool[key]]\n",
    "\n",
    "    # Pad the sequences to have the same length\n",
    "    \n",
    "    padded_input = [torch.cat((input_id, torch.zeros((max_len - len(input_id)), dtype=torch.long))) for input_id in input_ids]\n",
    "    padded_input = torch.stack(padded_input).to(torch.int64) # tensor\n",
    "\n",
    "    if key=='sentences_train':\n",
    "        input_id_train.append(padded_input)\n",
    "    else :\n",
    "        input_id_test.append(padded_input)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After transforming words into id, roberta model used. Word embedding can be achieved by keeping the gradient off. This means that PyTorch will not keep track of the operations that are involved in computing the word embeddings, and therefore it will not use them to compute the gradients during backpropagation. By using torch.no_grad() when computing the word embeddings, we can reduce the memory usage and computation time required during forward propagation, as well as avoid any potential errors that may occur if we accidentally try to compute gradients for the word embeddings. Additionally, disabling gradient tracking can also help to improve the speed and efficiency of the overall training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_train=input_id_train[0] # get tensor from list\n",
    "\n",
    "#  word embedding of train\n",
    "with torch.no_grad():\n",
    "    output = Roberta(input_id_train)\n",
    "    last_hidden_state_train = output[2] # Word embedding can get by input for next part like forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_test=input_id_test[0] # get tensor from list\n",
    "\n",
    "# word embedding of test\n",
    "with torch.no_grad():\n",
    "    output = Roberta(input_id_test)\n",
    "    last_hidden_state_test = output[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Roberta model has 12 encoders, and each encoder creates one embedding, giving a total of twelve embeddings. Every token has 769 bytes. Best encoding can be achieved by combining encoders; in this case, the sum of the last 7 encoders' embedding performed better for the classification task. Now, each sentence has a dimension of [1,47,768]. Entropy can help reduce dimension. Entropy converts the array into a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dimension of x_train is (without entorpy): torch.Size([1968, 47, 768])\n",
      "Final embedding shape x_train is (after entropy): torch.Size([1968, 47])\n",
      "----------\n",
      "Total dimension of x_test is (without entorpy): torch.Size([649, 47, 768])\n",
      "Final embedding shape x_test is (after entropy): torch.Size([649, 47])\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# find best embedding by combination of encoders and use entropy to convert sentence dimension from [1,47,768] to [1,768]\n",
    "\n",
    "'''\n",
    "Dimension: shape of whole dataset after embedding: [2617,47,768],\n",
    "           each sentence dimension: [1,47,768] shape \n",
    "           each word dimension: [1,768]\n",
    "'''\n",
    "x_train=[]\n",
    "x_test=[]\n",
    "embedding_pool={'x_train':last_hidden_state_train, 'x_test':last_hidden_state_test}\n",
    "\n",
    "for key in embedding_pool.keys():\n",
    "    final_embedding_sum=torch.stack(embedding_pool[key][-6:]).sum(0) # combine output of last six encoder\n",
    "    #--------------------------------------------\n",
    "    n_samples = final_embedding_sum.shape[0]\n",
    "    n_features = np.prod(final_embedding_sum.shape[1:])\n",
    "\n",
    "    X = final_embedding_sum.reshape(n_samples, n_features)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "\n",
    "   # Transform data using StandardScaler\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "   # Reshape scaled data back to original tensor shape\n",
    "    tensor_scaled = torch.tensor(X_scaled.reshape(n_samples, *final_embedding_sum.shape[1:]))\n",
    "\n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    print('Total dimension of', key, 'is (without entorpy):' ,tensor_scaled.shape)\n",
    "\n",
    "    final_embedding = torch.mean(tensor_scaled, dim=-1) # mean of last dimension\n",
    "\n",
    "    # entropy\n",
    "    # tensor_scaled=tensor_scaled-(torch.min(tensor_scaled))\n",
    "    # final_embedding = entropy(tensor_scaled, axis=-1) \n",
    "\n",
    "    print('Final embedding shape', key, 'is (after entropy):',final_embedding.shape) # one dimensional single sentence of length 768\n",
    "    print('----------')\n",
    "    if key=='x_train':\n",
    "        x_train.append(final_embedding)\n",
    "    else:\n",
    "        x_test.append(final_embedding)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first part has been completed successfully. Now onward, the classification part starts. Achieved embedding used for the training along with its label.\n",
    "\n",
    "- A Gaussian process regression model works better as it is a bit high-dimensional data. To overcome this uncertainty, Gaussian process regression works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification task\n",
    "x_train, x_test, y_train, y_test= x_train[0], x_test[0], df_train['label'], df_test['label']\n",
    "\n",
    "# Classification model: GaussianProcessClassifier \n",
    "Gaussian = GaussianProcessClassifier(kernel=1.0 * RBF(1.0),max_iter_predict=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gaussian.fit(x_train,y_train) # fit to the model\n",
    "y_pred=Gaussian.predict(x_test) # evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis\n",
    "\n",
    "- The classification results changed every time as the random state was set to zero while creating the test set. Result accuracy generally lied between 54 and 70% with the mean/entropy technique. Recall suggests how many items were correctly classified in that class. Precesion describes how accurate a model is at identifying relevant items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score :0.5469953775038521\n",
      "-----classification report-------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       355\n",
      "           1       0.50      0.00      0.01       294\n",
      "\n",
      "    accuracy                           0.55       649\n",
      "   macro avg       0.52      0.50      0.36       649\n",
      "weighted avg       0.53      0.55      0.39       649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy score (result vary as dataset shuffle each time:54%-68%)\n",
    "print(f\"Accuracy score :{metrics.accuracy_score(y_test,y_pred)}\")\n",
    "\n",
    "# classification report\n",
    "print('-----classification report-------')\n",
    "class_labels=  [0,1] # AD : 0, HC : 1\n",
    "print(classification_report(y_test, y_pred, labels=class_labels)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- In sum, it is important to select a good combination of encoders in order to get the best word embedding. The best word embedding is the main factor in good classification accuracy. Roberta is the most powerful variation model of BERT for embedding. This selection part is also challenging in the sense of making the classification model generalize because the combination of selected encoders might not be a good choice for every word. Word embedding is high-dimensional, and dimensionality can be reduced by using the mean or entropy function. Because of the high dimensionality of word embedding, the Gaussian process model might be more useful than others. The achieved accuracy range is 54%–70%, which can be improved by changing the classification model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc9c18c49ecff0086ebd087ab3cf18fc94c56d493e506b9bf33d86b94d3b3ea4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
