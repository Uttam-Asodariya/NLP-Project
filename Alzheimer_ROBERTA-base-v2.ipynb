{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstratct\n",
    "- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- For the creation of a better world, good mental health is required. Medical science progressed outstandingly, but still, some diseases exist which can not be identified earlier to curve them. Natural language processing (NLP) and deep learning (DL) combination can be used to identify such diseases by the classification algorithm.\n",
    "\n",
    "- Alzheimer's is one of the diseases which hard to detect in earlier phases or even in the present moment. Fortunately, Data science has the solution to it. A combination of natural language processing and deep learning can be used to detect the disease. Sentences can be transformed to word embedding by NLP. where the representation of words into vectors of real numbers. This can be done by BERT (Bidirectional Encoder Representations from Transformers). Berta has different variation such as ROBERTA, ELECTRA.\n",
    "\n",
    "- ROBERTA (Robustly Optimized BERT-Pretraining Approach) is powerful model for word embedding. It transform each sentences into token and these token is used by model for word embedding in encoder phase. \n",
    "\n",
    "- Gaussian process classification is useful for this high dimensional word embedding, as It has a huge uncertainty in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment for this notbook\n",
    "import torch\n",
    "\n",
    "from transformers import RobertaModel, RobertaModel, RobertaTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "from scipy.stats import entropy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "- There is two type of datasets. One with sentences and second with the patient health discription. First  type of data is used for this word embedding task. Data collected by image desciption of patients affected by Alzeihmer and helathy person.\n",
    "\n",
    "### Pre-processing\n",
    "\n",
    "- There is couple of csv file based on patients description and therefore It is important to convert them into appropriate formate for word embedding. Through combination of all csv file, make it single one and add label to identify which is Alzeihmer disease (AD) and healthy control(HC). Similarly, same process performed for test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all csv files of AD_train and add label column\n",
    "AD_train=[]\n",
    "path=r\"C:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\Data\\AD_train\"\n",
    "csv_files=glob.glob(path+'/*.csv')\n",
    "for f in csv_files:\n",
    "    df1 = pd.read_csv(f)\n",
    "    df1 = df1.loc[:, ~df1.columns.str.contains('^Unnamed')]\n",
    "    AD_train.append(df1)\n",
    "\n",
    "\n",
    "df_AD_train=pd.concat(AD_train,axis=0,ignore_index=True)\n",
    "label_AD=0 #AD : 0 label\n",
    "df_AD_train['label']=label_AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all csv files of AD_test and add label column\n",
    "AD_test=[]\n",
    "path=r\"C:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\Data\\AD_train\\AD_Test\"\n",
    "csv_files=glob.glob(path+'/*.csv')\n",
    "for f in csv_files:\n",
    "    df1 = pd.read_csv(f)\n",
    "    df1 = df1.loc[:, ~df1.columns.str.contains('^Unnamed')]\n",
    "    AD_test.append(df1)\n",
    "\n",
    "\n",
    "df_AD_test=pd.concat(AD_test,axis=0,ignore_index=True)\n",
    "label_AD=0 #AD : 0 label\n",
    "df_AD_test['label']=label_AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all csv files of HC_train and add label column\n",
    "HC_train=[]\n",
    "path=r\"C:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\Data\\HC_train\"\n",
    "csv_files=glob.glob(path+'/*.csv')\n",
    "for f in csv_files:\n",
    "    df2=pd.read_csv(f)\n",
    "    df2 = df2.loc[:, ~df2.columns.str.contains('^Unnamed')]\n",
    "    HC_train.append(df2)\n",
    "\n",
    "\n",
    "df_HC_train=pd.concat(HC_train,axis=0,ignore_index=True)\n",
    "label_HC=1 #HC : 1 label\n",
    "df_HC_train['label']=label_HC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all csv files of HC_test and add label column\n",
    "HC_test=[]\n",
    "path=r\"C:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\Data\\HC_train\\HC_Test\"\n",
    "csv_files=glob.glob(path+'/*.csv')\n",
    "for f in csv_files:\n",
    "    df2=pd.read_csv(f)\n",
    "    df2 = df2.loc[:, ~df2.columns.str.contains('^Unnamed')]\n",
    "    HC_test.append(df2)\n",
    "\n",
    "\n",
    "df_HC_test=pd.concat(HC_test,axis=0,ignore_index=True)\n",
    "label_HC=1 #HC : 1 label\n",
    "df_HC_test['label']=label_HC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatination of AD & HC train and similarly for test set\n",
    "df_train=pd.concat([df_AD_train,df_HC_train], ignore_index=True)\n",
    "df_test=pd.concat([df_AD_test,df_HC_test], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Furthermore, It is essential to shuffle data samples to avoid  overfitting and also help to create robust classification model. After pre-processing step, There is total 1968 samples in train set and 649 samples in test set. It is important to separate train and test set in starting to avoid mixture of sentences from one scipt to another one, because it is essential to maintain uniqness of patients' script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train 1968 . length of test 649\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I heard it might but I'm sayin(g) let's tell h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and the woman is standing in water .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the little girl's got her hand up for one .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wonder if he got a cookie .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>fell down .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>uhhuh .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>that's a mess .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>touching lip .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>doin(g) .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1968 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label\n",
       "0     I heard it might but I'm sayin(g) let's tell h...      0\n",
       "1                                                 yes .      0\n",
       "2                  and the woman is standing in water .      1\n",
       "3           the little girl's got her hand up for one .      1\n",
       "4                           wonder if he got a cookie .      0\n",
       "...                                                 ...    ...\n",
       "1963                                        fell down .      0\n",
       "1964                                            uhhuh .      1\n",
       "1965                                    that's a mess .      0\n",
       "1966                                     touching lip .      1\n",
       "1967                                          doin(g) .      0\n",
       "\n",
       "[1968 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffling\n",
    "df_train = df_train.sample(frac=1,random_state=0,replace=True).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1,random_state=0,replace=True).reset_index(drop=True)\n",
    "print('length of train', len(df_train),'.','length of test', len(df_test)) # output of train and test length\n",
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "- The Next step is word embedding of sentences. For the word embedding, Tokenizer and Roberta model required. Tokenizer is helpful to tokenize words of sentenc. Roberta model use 'G' character to represt space as described in given below example. \n",
    "\n",
    "- Example: 'My name is Khodiyar'\n",
    "           ['My','Gname', 'Gis', 'GKhodiyar'] => Tokenized sentence\n",
    "\n",
    "- Transformer convert each tokenized word into unique id which can be taken by roberta model for further process. Here, tokenizer add two addition tokens in starting and ending of each sentence. For the distinguish each sentence to each other.\n",
    "\n",
    "- For more convenience in this task we make same length for all sentences by taking into account length of longest sentence wich is 47 in our case. Extra added tokens are zero which will not affect to the model. Roberta model config can be checked by **model.config**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# loading of tokenizer and roberta model\n",
    "tokenizer=RobertaTokenizer.from_pretrained('roberta-base')\n",
    "Roberta = RobertaModel.from_pretrained('roberta-base',output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train=df_train['sentence'] # train\n",
    "sentences_test=df_test['sentence']  # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_pool={'sentences_train':sentences_train,'sentences_test':sentences_test} # train and test pool\n",
    "input_id_train=[] # list\n",
    "input_id_test=[]  # list\n",
    "max_len = 47\n",
    "\n",
    "# convert sentence into token\n",
    "for key in sentences_pool.keys():\n",
    "    input_ids = [torch.tensor(tokenizer.encode(sentence, add_special_tokens=True)) for sentence in sentences_pool[key]]\n",
    "\n",
    "    # Pad the sequences to have the same length\n",
    "    \n",
    "    padded_input = [torch.cat((input_id, torch.zeros((max_len - len(input_id)), dtype=torch.long))) for input_id in input_ids]\n",
    "    padded_input = torch.stack(padded_input).to(torch.int64) # tensor\n",
    "\n",
    "    if key=='sentences_train':\n",
    "        input_id_train.append(padded_input)\n",
    "    else :\n",
    "        input_id_test.append(padded_input)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After transforming words into id, roberta model used. Word embedding can be achieved by keeping gradient off. This means that PyTorch will not keep track of the operations that are involved in computing the word embeddings, and therefore it will not use them to compute the gradients during backpropagation. By using torch.no_grad() when computing the word embeddings, we can reduce the memory usage and computation time required during forward propagation, as well as avoid any potential errors that may occur if we accidentally try to compute gradients for the word embeddings. Additionally, disabling gradient tracking can also help to improve the speed and efficiency of the overall training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_train=input_id_train[0] # get tensor from list\n",
    "\n",
    "#  word embedding of train\n",
    "with torch.no_grad():\n",
    "    output = Roberta(input_id_train)\n",
    "    last_hidden_state_train = output[2] # Word embedding can get by input for next part like forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_test=input_id_test[0] # get tensor from list\n",
    "\n",
    "# word embedding of test\n",
    "with torch.no_grad():\n",
    "    output = Roberta(input_id_test)\n",
    "    last_hidden_state_test = output[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Roberta model has 12 encoder and each encoder create one embedding, It gives total twelve embeddings. Every token has 769 length. Best encodding can be achieved by combination of encoders, in this case sum of last 7 encoders' embedding performed better for classification task. Now, each sentence has dimension of **[1,47,768]**. Entropy can help to reduce dimension. Entropy convert array into single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value of x_train is: tensor(-84.2695)\n",
      "Total dimension of x_train is (without entorpy): torch.Size([1968, 47, 768])\n",
      "Final embedding shape x_train is (after entropy): (1968, 768)\n",
      "----------\n",
      "Min value of x_test is: tensor(-81.4422)\n",
      "Total dimension of x_test is (without entorpy): torch.Size([649, 47, 768])\n",
      "Final embedding shape x_test is (after entropy): (649, 768)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# find best embedding by combination of encoders and use entropy to convert sentence dimension from [1,47,768] to [1,768]\n",
    "\n",
    "'''\n",
    "Dimension: shape of whole dataset after embedding: [2617,47,768],\n",
    "           each sentence dimension: [1,47,768] shape \n",
    "           each word dimension: [1,768]\n",
    "'''\n",
    "x_train=[]\n",
    "x_test=[]\n",
    "embedding_pool={'x_train':last_hidden_state_train, 'x_test':last_hidden_state_test}\n",
    "\n",
    "for key in embedding_pool.keys():\n",
    "    final_embedding=torch.stack(embedding_pool[key][-7:]).sum(0) # combine output of last six encoder\n",
    "    print('Min value of', key,'is:', torch.min(final_embedding))\n",
    "    final_embedding=final_embedding+torch.max(final_embedding) # deviation from negative to positive for entropy\n",
    "    print('Total dimension of', key, 'is (without entorpy):' ,final_embedding.shape)\n",
    "    # normalized=normalize(x, p=1, dim=0)\n",
    "    # print(normalized.shape)\n",
    "    final_embedding = entropy(final_embedding, axis=1) \n",
    "    print('Final embedding shape', key, 'is (after entropy):',final_embedding.shape) # one dimensional single sentence of length 768\n",
    "    print('----------')\n",
    "    if key=='x_train':\n",
    "        x_train.append(final_embedding)\n",
    "    else:\n",
    "        x_test.append(final_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\uttam\\OneDrive\\Desktop\\NLP Project\\NLP-Project\\Alzheimer_ROBERTA-base-v2.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/uttam/OneDrive/Desktop/NLP%20Project/NLP-Project/Alzheimer_ROBERTA-base-v2.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m final_embedding\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "final_embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification task\n",
    "x_train, x_test, y_train, y_test= x_train[0], x_test[0], df_train['label'], df_test['label']\n",
    "\n",
    "# Classification model: GaussianProcessClassifier \n",
    "Gaussian = GaussianProcessClassifier(kernel=1.0 * RBF(1.0),max_iter_predict=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gaussian.fit(x_train,y_train) # fit to the model\n",
    "y_pred=Gaussian.predict(x_test) # evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print score for :0.6024653312788906\n"
     ]
    }
   ],
   "source": [
    "# accuracy score (result vary as dataset shuffle each time:54%-68%)\n",
    "print(f\"print score for :{metrics.accuracy_score(y_test,y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.60      0.62       355\n",
      "           1       0.56      0.60      0.58       294\n",
      "\n",
      "    accuracy                           0.60       649\n",
      "   macro avg       0.60      0.60      0.60       649\n",
      "weighted avg       0.61      0.60      0.60       649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "class_labels=  [0,1] # AD : 0, HC : 1\n",
    "print(classification_report(y_test, y_pred, labels=class_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc9c18c49ecff0086ebd087ab3cf18fc94c56d493e506b9bf33d86b94d3b3ea4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
